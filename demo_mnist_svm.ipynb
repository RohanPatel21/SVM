{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: SVM for MNIST Digit Recognition\n",
    "\n",
    "In this demo, you will learn to:\n",
    "* Load and display images\n",
    "* Formulate image classification problems\n",
    "* Explain the limitations of linear classifiers for image classification\n",
    "* Build a simple SVM image classifier \n",
    "* Save and load results using `pickle`.\n",
    "\n",
    "For data, we will use the classic [MNIST](https://en.wikipedia.org/wiki/MNIST_database) data set used to recognize hand-written digits.  The dataset was originally produced in the 1980s and is now widely-used in machine learning classes as a simple image classification problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we load the standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the MNIST dataset is so widely-used, it comes in as a build-in dataset in many packages. In this lab, we will download the data from the Tensorflow package -- See here for [installations instructions](https://www.tensorflow.org/install).  You can get the data from other sources as well. \n",
    "\n",
    "In the Tensorflow dataset, the training and test data are represented as arrays:\n",
    "\n",
    "     Xtr.shape = 60000 x 28 x 28\n",
    "     Xts.shape = 10000 x 28 x 28\n",
    "     \n",
    "The test data consists of `60000` images of size `28 x 28` pixels; the test data consists of `10000` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(Xtr,ytr),(Xts,yts) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Xtr shape: %s' % str(Xtr.shape))\n",
    "print('Xts shape: %s' % str(Xts.shape))\n",
    "\n",
    "ntr = Xtr.shape[0]\n",
    "nts = Xts.shape[0]\n",
    "nrow = Xtr.shape[1]\n",
    "ncol = Xtr.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel value is from `[0,255]`.  For this lab, it will be convenient to recale the value to -1 to 1 and reshape it as a `ntr x npix` and `nts x npix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = 2*(Xtr/255 - 0.5)\n",
    "Xtr = Xtr.reshape((ntr,npix))\n",
    "\n",
    "Xts = 2*(Xts/255 - 0.5)\n",
    "Xts = Xts.reshape((nts,npix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_digit(ax, x):\n",
    "    nrow = 28\n",
    "    ncol = 28\n",
    "    xsq = x.reshape((nrow,ncol))\n",
    "    ax.imshow(xsq,  cmap='Greys_r')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Select random digits\n",
    "nplt = 8\n",
    "Iperm = np.random.permutation(ntr)\n",
    "\n",
    "# Plot the images using the subplot command\n",
    "fig, ax = plt.subplots(1,nplt,figsize=(10,3))\n",
    "for i in range(nplt):\n",
    "    ind = Iperm[i]\n",
    "    plt_digit(ax[i], Xtr[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a Logistic Regression Classifier\n",
    "\n",
    "To classify the digits, we will first use a logistic classifier.  We select a small number of samples for training. Generally, you would use more training samples, but the optimizer is very slow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr1 = 5000\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the optimizer.  I have placed the `verbose=1` option so that you can see the progress.  It may not appear in the browser but in the command line where you launched jupyter notebook.  This can take several minutes and will likely say that it ran out of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(verbose=1, solver='lbfgs',\\\n",
    "                                         max_iter=500)\n",
    "logreg.fit(Xtr1,ytr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before continuing, since it takes so long to run the optimizer, let's save the results in a file.  You can use `pickle` module for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( \"mnist_logreg.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [logreg, Xtr1, ytr1, Iperm],  fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can recover the objects via the `pickle.load` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"mnist_logreg.p\", \"rb\" ) as fp:\n",
    "    logreg, Xtr1, ytr1, Iperm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can measure the accuracy on the test data.  Again, we will test on a small sub-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts1 = 5000\n",
    "Iperm_ts = np.random.permutation(nts) \n",
    "Xts1 = Xts[Iperm_ts[:nts1],:]\n",
    "yts1 = yts[Iperm_ts[:nts1]]\n",
    "yhat = logreg.predict(Xts1)\n",
    "acc = np.mean(yhat == yts1)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 89%.  This may sounds OK, but it is actually very poor.  Had we increased the number of training samples, it would have improved to about 92%, but that is still very bad.  To illustrate, let's plot some of the errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "Ierr = np.where(yts1 != yhat)[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,nplt,figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    ind = Ierr[i]    \n",
    "    plt_digit(ax[i], Xts1[ind,:])        \n",
    "    title = 'true={0:d} est={1:d}'.format(yts1[ind].astype(int), yhat[ind].astype(int))\n",
    "    ax[i].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, some of these digits are very easy to classify for a human.  We can get more fine-grained analysis of the digit errors by computing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "C = confusion_matrix(yts1,yhat)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "Csum = np.sum(C,1)\n",
    "C = C / Csum[None,:]\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(np.array_str(C, precision=3, suppress_small=True))\n",
    "plt.imshow(C, interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Weights\n",
    "\n",
    "To see the problem with the logistic classifier, it is useful to plot the weights for each digit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = logreg.coef_\n",
    "ndigit = W.shape[0]\n",
    "fig, ax = plt.subplots(2,5,figsize=(10, 5))\n",
    "for i in range(ndigit):\n",
    "    irow = i // 5\n",
    "    icol = i % 5\n",
    "    plt_digit(ax[irow,icol], W[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see is that each weight is a very blurry version of the digit.  The blurriness is due to the fact that weight must correlate with all shifts, rotations and other variations of the digits.  As a result, the weights begin to correlate with other incorrect digits leading to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this problem more clearly, the code below does the following:\n",
    "\n",
    "*  Takes `nplot=8` random training digits with the label 2 and `nplot=8` that are not the digit 2.\n",
    "*  Finds a weight `w` to the first of the digits with the label 2.\n",
    "*  Plots the all the digits `x` and their correlation with the weight `z = w.dot(x)`\n",
    "\n",
    "We see that the correlation `z` is not consistently higher for the digits 2 instead of the non-digit 2.\n",
    "Therefore, if we try to use a linear classifier with this weight we would make a lot of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find digits with 2 and digits that are not 2\n",
    "idig = 2\n",
    "Idig   = np.where(ytr==idig)[0]\n",
    "Inodig = np.where(ytr!=idig)[0]\n",
    "n = len(Idig)\n",
    "Idig = Idig[np.random.permutation(n)]\n",
    "n = len(Inodig)\n",
    "Inodig = Inodig[np.random.permutation(n)]\n",
    "\n",
    "# Set the weight to the first digit\n",
    "w = Xtr[Idig[0],:]\n",
    "w = w / (np.sum(w**2))\n",
    "\n",
    "# Find the correlation coefficient with the other digits 7\n",
    "nplot = 8\n",
    "fig, ax = plt.subplots(2,nplot,figsize=(12, 6))\n",
    "rho = np.zeros(nplot)\n",
    "for i in range(nplot):\n",
    "    # Get a true and false digit\n",
    "    j0 = Idig[i]\n",
    "    j1 = Inodig[i]\n",
    "    x0 = Xtr[j0,:]\n",
    "    x1 = Xtr[j1,:]\n",
    "    \n",
    "    # Compute the correlations with the two digits\n",
    "    rho0 = w.dot(x0)\n",
    "    rho1 = w.dot(x1)\n",
    "    \n",
    "    plt_digit(ax[0,i], x0)    \n",
    "    ax[0,i].set_title('%5.3f' % rho0)\n",
    "    plt_digit(ax[1,i], x1)\n",
    "    ax[1,i].set_title('%5.3f' % rho1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercise\n",
    "\n",
    "Let's try to build a simple linear classifier to detect if a digit is a 2 or not.\n",
    "*  Set the binary label `q[i] = 1` if `ytr[i] == 2` and `q[i] = 0` if `ytr[i] != 2`.\n",
    "*  Set the weight `w` = average of `Xtr[i,:]`, the `q[i] == 1`.\n",
    "*  Plot `w`.  You can use the `plt_digit` command with `ax = plt.gca()`\n",
    "*  Compute `z[i] = Xtr[i,:].dot(w)`.  This is the linear correlation of the weight with all the training digits.\n",
    "*  Plot the CDF of `z[i]` for `q[i] = 0` and the CDF of `z[i]` for `q[i] = 1`.\n",
    "*  Consider the classifier:\n",
    "    \n",
    "       qhat[i] = 1,   if z[i] > t\n",
    "               = 0,   if z[i] <= t\n",
    "          \n",
    "   For some threshold `t`.  What does the above CDFs tell you about the accuracy achievable with this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an SVM classifier\n",
    "\n",
    "We now try an SVM classifier.  The parameters are given by \n",
    "\n",
    "https://martin-thoma.com/svm-with-sklearn/\n",
    "\n",
    "This website has a nice summary of the main equations for SVM as well.  That site trained on 40000 samples and tested on 20000.  But, to make this run faster, we will train on 10000 and test on 10000.  If you increase to 40000 training samples, you can get past 99% accuracy.\n",
    "\n",
    "First, we import the SVM package and construct the SVC with the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let us try the linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "# svc = svm.SVC(probability=False,  kernel=\"rbf\", C=2.8, gamma=.0073,verbose=10)\n",
    "svc = svm.SVC(probability=False,  kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the training and test data.  The features are re-scaled to be between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the training data.  Again, this will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr1 = 5000\n",
    "nts1 = 5000\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]\n",
    "Xts1 = Xts[Iperm_ts[:nts1],:]\n",
    "yts1 = yts[Iperm_ts[:nts1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(Xtr,ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results in case you want them without re-running the above the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( \"mnist_svc.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [svc, Xtr1,ytr1], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this step if you run the classifier directly\n",
    "import pickle\n",
    "with open( \"mnist_svc.p\", \"rb\" ) as fp:\n",
    "    svc, Xtr1,ytr1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy on the test data.  The prediction can take several minutes too -- SVMs are *very* slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_ts = svc.predict(Xts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since even the prediction (sometimes called inference) is slow with SVMs, we will save the results in `pickle` file. Instead of running the prediction again, you can recapture the data with the following comamnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_svc_test.p\", \"wb\") as fp:\n",
    "    pickle.dump([Xts1,yts1,yhat_ts], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_svc_test.p\", \"rb\") as fp:\n",
    "    Xts1,yts1,yhat_ts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(yhat_ts == yts1)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 93%.  Again, had you trained on all 50,000 samples, it would have been much better -- close to 98.5%.  But, even this result is much better than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Support Vectors\n",
    "\n",
    "Let's take a look at the support vectors.  We see there about over 2500 support vectors.  So, about quarter the training samples were used as SVs.  This is partly why the prediction was so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = svc.support_vectors_\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "nsv = S.shape[0]\n",
    "Iperms = np.random.permutation(nsv)\n",
    "fig, ax = plt.subplots(1,nplt,figsize=(10, 4))\n",
    "for i in range(nplt):                \n",
    "    ind = Iperms[i]\n",
    "    plt_digit(ax[i], S[ind,:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the support vectors look like digits we want to recognize. They are like the 'match filters'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us see the performanc of SVM using the RBF kernel\n",
    "\n",
    "The main benefit of the SVMs is that they can provide nonlinear classification rules.  We can do this with a RBF kernel.  We first fit this as before -- again this will take a long time!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "svcrbf = svm.SVC(probability=False,  kernel=\"rbf\", C=2.8, gamma=.0073,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this step if loading from previous result \n",
    "svcrbf.fit(Xtr1,ytr1)\n",
    "import pickle\n",
    "with open( \"mnist_svcrbf.p\", \"wb\" ) as fp:\n",
    "    pickle.dump( [svcrbf, Xtr1, ytr1], fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip if you run the previous cell\n",
    "import pickle\n",
    "with open( \"mnist_svcrbf.p\", \"rb\" ) as fp:\n",
    "    svcrbf, Xtr1, ytr1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions and save the results and measure the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if load from saved result\n",
    "yhat_ts = svcrbf.predict(Xts1)\n",
    "\n",
    "with open(\"mnist_svcrbf_test.p\", \"wb\") as fp:\n",
    "    pickle.dump([yhat_ts,yts1,Xts1], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if you run the previouse cell\n",
    "with open(\"mnist_svcrbf_test.p\", \"rb\") as fp:\n",
    "    yhat_ts,yts1,Xts1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(yhat_ts == yts1)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RBF kernel provides more accurate results. \n",
    "Now let us plot some errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ierr = np.where((yhat_ts != yts1))[0]\n",
    "nplt = 4\n",
    "fig, ax = plt.subplots(1,nplt,figsize=(10, 4))\n",
    "for i in range(nplt):             \n",
    "    ind = Ierr[i]    \n",
    "    plt_digit(ax[i], Xts1[ind,:])        \n",
    "    title = 'true={0:d} est={1:d}'.format(yts1[ind].astype(int), yhat_ts[ind].astype(int))\n",
    "    ax[i].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that again a human would not have made these errors, but the digits in error are much less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Support Vectors\n",
    "\n",
    "Let's take a look at the support vectors.  We see there about 5000 support vectors.  So, about half the training samples were used as SVs, more than that for the linear kernel.  This is partly why the prediction was so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = svcrbf.support_vectors_\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can plot some of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplt = 4\n",
    "nsv = S.shape[0]\n",
    "Iperms = np.random.permutation(nsv)\n",
    "fig, ax = plt.subplots(1,nplt,figsize=(10, 4))\n",
    "for i in range(nplt):        \n",
    "    ind = Iperms[i]\n",
    "    plt_digit(ax[i], S[ind,:])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this demo, we specified the parameters for the SVC. In the lab, you will be asked to find the optimal parameters through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
